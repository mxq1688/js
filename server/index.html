<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>

<body>
    <div onclick="fetchAudio()">fetchAudio</div>
    <div onclick="mediaAudio()">mediaAudio</div>
    <div onclick="audioLabel()">audioLabel</div>
    <audio id="audio2" muted autoplay controls></audio>
    <canvas id="canvas"></canvas>
    <audio id="audio" autoplay controls src="http://localhost:3000/1.mp3"></audio>
    <!-- <video id="video" autoplay controls src="http://localhost:3000/1.mp4"></video> -->

    <script>
        let mediaStream
        var audioContext
        navigator.mediaDevices.getUserMedia({ audio: true })
            .then(function (stream) {
                audioContext = new (window.AudioContext || window.webkitAudioContext())();
                /* 使用这个 stream */
                mediaStream = stream
                audio2.srcObject = stream
            })
            .catch(function (err) {
                /* 处理 error */
                switch (error.message || error.name) {
                    case 'PERMISSION_DENIED':
                    case 'PermissionDeniedError':
                        console.info('用户拒绝提供信息。')
                        break
                    case 'NOT_SUPPORTED_ERROR':
                    case 'NotSupportedError':
                        console.info('浏览器不支持硬件设备。')
                        break
                    case 'MANDATORY_UNSATISFIED_ERROR':
                    case 'MandatoryUnsatisfiedError':
                        console.info('无法发现指定的硬件设备。')
                        break
                    default:
                        console.info('无法打开麦克风。异常信息:' + (error.code || error.name))
                        break
                }
            });

        // 失真
        function makeDistortionCurve(amount) {
            var k = typeof amount === "number" ? amount : 50,
            n_samples = 44100,
            curve = new Float32Array(n_samples),
            deg = Math.PI / 180,
            i = 0,
            x;
            for (; i < n_samples; ++i) {
            x = (i * 2) / n_samples - 1;
            curve[i] = ((3 + k) * x * 20 * deg) / (Math.PI + k * Math.abs(x));
            }
            return curve;
        }
        async function fetchAudio() {
            let source = audioContext.createBufferSource(); // 创建音频源头节点
            const res = await fetch('http://localhost:3000/1.mp4');
            const arrayBuffer = await res.arrayBuffer(); // byte array字节数组
            // const audioBuffer = await audioContext.decodeAudioData(arrayBuffer, function (decodeData) {
            //     return decodeData;
            // });
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer).then((decodeData) => {
                return decodeData;
            })
            source.buffer = audioBuffer; // 设置数据
            source.loop = true; //设置，循环播放

            const gainNode = audioContext.createGain();
            gainNode.gain.value = 1
            source.connect(gainNode);
            const biquadFilter = audioContext.createBiquadFilter(); //滤波节点，过滤指定频段波形
            gainNode.connect(biquadFilter)

            const waveShaper = audioContext.createWaveShaper();

            biquadFilter.connect(waveShaper);

            const convolver = audioContext.createConvolver(); //混音节点
            convolver.buffer = audioBuffer

            waveShaper.connect(convolver)

            let analyser = audioContext.createAnalyser(); // 创建AnalyserNode节点
            analyser.fftSize = 256;

            convolver.connect(analyser);
            analyser.connect(audioContext.destination);

            // 可以对音频做任何控制
            source.start(0); //立即播放
            flushData(analyser)
            
        }
        function flushData(analyser) {
            var globalID;
            function render() {
                let dataArray = new Uint8Array(analyser.frequencyBinCount); //analyser.frequencyBinCount为analyser.fftSize设置的一半
                // 获取时域数据（对于波形显示，通常使用getByteTimeDomainData）
                // analyser.getByteTimeDomainData(dataArray);
                // console.log("Time domain data:", dataArrayTime);

                // 获取频域数据
                analyser.getByteFrequencyData(dataArray);
                console.log("Frequency domain data:", dataArray);
                //Todo 通过dataArray数据实现可视化效果
                // 可控制canvas css

                globalID = requestAnimationFrame(render);
            };
            globalID = requestAnimationFrame(render);
        }


        function mediaAudio() {

            // 创建一个与MediaStream关联的Source Node
            // const sourceNode = audioContext.createMediaStreamSource(mediaStream);
            const sourceNode = audioContext.createMediaStreamSource(audio2.captureStream());
            // 这里可以添加更多音频处理节点，比如增益控制、分析等
            // 示例：添加一个增益节点调节音量
            const gainNode = audioContext.createGain();
            gainNode.gain.value = 0.1
            sourceNode.connect(gainNode);
            let analyser = audioContext.createAnalyser(); // 创建AnalyserNode节点
            analyser.fftSize = 256;
            gainNode.connect(analyser);
            analyser.connect(audioContext.destination);
            flushData(analyser)
            // 开始音频处理流程
            // 注意：如果是处理实时流，不需要手动开始，连接后自动播放
        }

        function audioLabel(){
            const sourceNode = audioContext.createMediaElementSource(audio);
            // 将媒体元素源节点连接到输出目的地，通常是扬声器或耳机
            // sourceNode.connect(audioContext.destination);

            const gainNode = audioContext.createGain();
            gainNode.gain.value = 0.1
            sourceNode.connect(gainNode);
            audio.volume = 0.5

            let analyser = audioContext.createAnalyser(); // 创建AnalyserNode节点
            analyser.fftSize = 256;
            gainNode.connect(analyser);
            analyser.connect(audioContext.destination);
            flushData(analyser)
            audio.play()
        }


    </script>
</body>

</html>