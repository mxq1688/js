<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>

<body>
    <div onclick="fetchAudio()">fetchAudio</div>
    <div onclick="mediaAudio()">mediaAudio</div>
    <div onclick="audioLabel()">audioLabel</div>
    <audio id="audio2" muted autoplay controls></audio>
    <video id="video" muted autoplay controls style="width: 150px;height: 150px;"></video>
    <!-- <canvas id="canvas"></canvas> -->
    <audio id="audio" autoplay controls src="http://localhost:3000/1.mp3"></audio>
    <!-- <video id="video" autoplay controls src="http://localhost:3000/1.mp4"></video> -->

    <div>
        <div>MediaRecorder</div>
        <button onclick="mStart()">录制开始</button>
        <button onclick="mPause()">暂停</button>
        <button onclick="mResume()">继续</button>
        <button onclick="mStop()">录制结束</button>
        <button onclick="mRequest()">请求录制数据</button>
    </div>
    <div>
        <div>AudioRecorder</div>
        <button onclick="aStart()">录制开始</button>
        <button onclick="aPause()">暂停</button>
        <button onclick="aResume()">继续</button>
        <button onclick="aStop()">录制结束</button>
    </div>
    <script type="module">
        import Resampler from './resampler.js'
        window.Resampler = Resampler
    </script>

    <script>
        let mediaStream
        var audioContext
        let mediaRecorder
        let audioRecorder
        let audioSource
        navigator.mediaDevices.getUserMedia({ audio: true })
            .then(function (stream) {
                audioContext = new (window.AudioContext || window.webkitAudioContext())();
                /* 使用这个 stream */
                mediaStream = stream
                audio2.srcObject = stream
                video.srcObject = stream

                mediaRecorderInit()
                audioRecorderInit()
            })
            .catch(function (err) {
                console.log(err);
                /* 处理 error */
                switch (error.message || error.name) {
                    case 'PERMISSION_DENIED':
                    case 'PermissionDeniedError':
                        console.info('用户拒绝提供信息。')
                        break
                    case 'NOT_SUPPORTED_ERROR':
                    case 'NotSupportedError':
                        console.info('浏览器不支持硬件设备。')
                        break
                    case 'MANDATORY_UNSATISFIED_ERROR':
                    case 'MandatoryUnsatisfiedError':
                        console.info('无法发现指定的硬件设备。')
                        break
                    default:
                        console.info('无法打开麦克风。异常信息:' + (error.code || error.name))
                        break
                }
            });

        let recordedChunks = [];
        function audioRecorderInit() {
            audioSource = audioContext.createMediaStreamSource(mediaStream)

            const createScript =
                audioContext.createScriptProcessor || audioContext.createJavaScriptNode;
            audioRecorder = createScript.apply(audioContext, [4096, 1, 1]);

            // 当有音频数据时，调用此函数
            audioRecorder.onaudioprocess = function (event) {
                // event.inputBuffer 是一个 AudioBuffer 对象，包含 PCM 数据
                const buffer = event.inputBuffer.getChannelData(0);

                // console.log(buffer);
                // 将 buffer 中的 PCM 数据复制到一个新的 Float32Array 中
                const array = new Float32Array(buffer.length);
                array.set(buffer);
                recordedChunks.push(array);

                audioData.input(buffer);
                const resampledWavData = audioData.resample(buffer);
                // console.log(buffer, resampledWavData, audioContext.sampleRate);
                // const resampled16BitsWavData = audioData.quantize(resampledWavData, 16);
                // console.log(resampled16BitsWavData.buffer);
            };

        }

        function aStart() {
            audioSource.connect(audioRecorder);
            audioRecorder.connect(audioContext.destination);
        }
        function aPause() {
            audioRecorder.disconnect();
            audioContext.suspend();
        }
        function aResume() {
            audioContext.resume();
            audioSource.connect(audioRecorder);
            audioRecorder.connect(audioContext.destination);
        }
        function aStop() {
            audioRecorder.disconnect();
            audioContext.close();
            mediaStream?.getTracks().forEach((t) => t.stop());
            dellAudioData()
        }
        const audioData = {
            size: 0, // 录音文件长度
            buffer: [], // 录音缓存, Array of Float32Array
            input: function (data) {
                this.buffer.push(new Float32Array(data)); //?
                this.size += data.length;
            },
            resample: function (buffer, newSampleRate = 24000) {
                const resampler = new Resampler(
                    audioContext.sampleRate,
                    newSampleRate,
                    1,
                    buffer.length
                );
                return resampler.resample(buffer);
            },

            resampleAndEncodeWAV: function (
                sampleRate = 48000,
                bits = 16,
                start = 0.0,
                end = -1
            ) {
                // Resample
                let bytes = this.buffer
                    .map((buf) => this.resample(buf, sampleRate))
                    .reduce((pre, aft) => {
                        return [...pre, ...aft];
                    }, []);

                // Trim the wav
                const startIdx = sampleRate * start;
                let endIdx = bytes.length;
                if (end >= 0) {
                    endIdx = sampleRate * end;
                }
                bytes = bytes.slice(startIdx, endIdx);

                // Write wav file
                const dataLength = bytes.length * (bits / 8);
                const buffer = new ArrayBuffer(44 + dataLength);
                const data = new DataView(buffer);

                const channelCount = 1; // 单声道
                let offset = 0;

                const writeString = function (str) {
                    for (let i = 0; i < str.length; i++) {
                        data.setUint8(offset + i, str.charCodeAt(i));
                    }
                };

                // 资源交换文件标识符
                writeString("RIFF");
                offset += 4;
                // 下个地址开始到文件尾总字节数,即文件大小-8
                data.setUint32(offset, 36 + dataLength, true);
                offset += 4;
                // WAV文件标志
                writeString("WAVE");
                offset += 4;
                // 波形格式标志
                writeString("fmt ");
                offset += 4;
                // 过滤字节,一般为 0x10 = 16
                data.setUint32(offset, 16, true);
                offset += 4;
                // 格式类别 (PCM形式采样数据)
                data.setUint16(offset, 1, true);
                offset += 2;
                // 通道数
                data.setUint16(offset, channelCount, true);
                offset += 2;
                // 采样率,每秒样本数,表示每个通道的播放速度
                data.setUint32(offset, sampleRate, true);
                offset += 4;
                // 波形数据传输率 (每秒平均字节数) 单声道×每秒数据位数×每样本数据位/8
                data.setUint32(offset, channelCount * sampleRate * (bits / 8), true);
                offset += 4;
                // 快数据调整数 采样一次占用字节数 单声道×每样本的数据位数/8
                data.setUint16(offset, channelCount * (bits / 8), true);
                offset += 2;
                // 每样本数据位数
                data.setUint16(offset, bits, true);
                offset += 2;
                // 数据标识符
                writeString("data");
                offset += 4;
                // 采样数据总数,即数据总大小-44
                data.setUint32(offset, dataLength, true);
                offset += 4;
                // 写入采样数据
                if (bits === 8) {
                    for (let i = 0; i < bytes.length; i++, offset++) {
                        const s = Math.max(-1, Math.min(1, bytes[i]));
                        let val = s < 0 ? s * 0x8000 : s * 0x7fff;
                        val = 255 / (65535 / (val + 32768));
                        data.setInt8(offset, val);
                    }
                } else {
                    for (let i = 0; i < bytes.length; i++, offset += 2) {
                        const s = Math.max(-1, Math.min(1, bytes[i]));
                        data.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
                    }
                }

                return new Blob([data], { type: "audio/wav" });
            },

            quantize: function (buffer, bits = 16) {
                const dataLength = buffer.length * (bits / 8);
                const data = new DataView(new ArrayBuffer(dataLength));

                // this.outputBuffer = new int16Array(this.buffer.length)
                let offset = 0;
                if (bits === 8) {
                    for (let i = 0; i < buffer.length; i++, offset++) {
                        const s = Math.max(-1, Math.min(1, buffer[i]));
                        let val = s < 0 ? s * 0x8000 : s * 0x7fff;
                        val = 255 / (65535 / (val + 32768));
                        data.setInt8(offset, val);
                    }
                } else {
                    for (let i = 0; i < buffer.length; i++, offset += 2) {
                        const s = Math.max(-1, Math.min(1, buffer[i]));
                        data.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
                    }
                }
                return data;
            },
        };
        function dellAudioData() {
            // 创建一个完整的 AudioBuffer，用于保存录制的音频
            // const recordingLength = recordedChunks.reduce((acc, chunk) => acc + chunk.length, 0);
            // console.log(recordedChunks, recordingLength, recordedChunks.length, audioContext.sampleRate);
            // const audioBuffer = audioContext.createBuffer(1, recordingLength, audioContext.sampleRate);

            // let srcStart = 0;
            // for (const chunk of recordedChunks) {
            //     audioBuffer.copyToChannel(chunk, 0, srcStart);
            //     srcStart += chunk.length;
            // }

        }

        function mediaRecorderInit() {
            const mimeType = 'video/webm;codecs=vp9';
            mediaRecorder = new MediaRecorder(mediaStream, { mimeType });
            console.log('isTypeSupported', MediaRecorder.isTypeSupported(mimeType), mediaRecorder.mimeType);
            const chunks = []
            mediaRecorder.ondataavailable = function (event) {
                if (event.data && event.data.size > 0) {
                    console.log(event.data, 'data');
                    chunks.push(event.data); // 将数据块添加到数组中
                }
            };
            mediaRecorder.onstop = function () {
                const blob = new Blob(chunks, { type: chunks[0].type }); // 将数据块组合成 Blob
                console.log(blob, 111);
                const url = URL.createObjectURL(blob); // 创建 Blob 的 URL
                const a = document.createElement('a'); // 创建一个 <a> 元素用于下载
                a.href = url;
                a.download = 'recording.webm';
                a.click(); // 触发下载
            }
        }
        function mStart() {
            mediaRecorder.start(3000);
        }
        function mPause() {
            mediaRecorder.pause();
        }
        function mResume() {
            mediaRecorder.resume();
        }
        function mStop() {
            mediaStream?.getTracks().forEach((t) => t.stop());
            mediaRecorder.stop();
        }
        function mRequest() {
            mediaRecorder.requestData();
        }
        // 失真
        function makeDistortionCurve(amount) {
            var k = typeof amount === "number" ? amount : 50,
                n_samples = 44100,
                curve = new Float32Array(n_samples),
                deg = Math.PI / 180,
                i = 0,
                x;
            for (; i < n_samples; ++i) {
                x = (i * 2) / n_samples - 1;
                curve[i] = ((3 + k) * x * 20 * deg) / (Math.PI + k * Math.abs(x));
            }
            return curve;
        }
        async function fetchAudio() {
            let source = audioContext.createBufferSource(); // 创建音频源头节点
            const res = await fetch('http://localhost:3000/1.mp4');
            const arrayBuffer = await res.arrayBuffer(); // byte array字节数组
            // const audioBuffer = await audioContext.decodeAudioData(arrayBuffer, function (decodeData) {
            //     return decodeData;
            // });
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer).then((decodeData) => {
                return decodeData;
            })
            source.buffer = audioBuffer; // 设置数据
            source.loop = true; //设置，循环播放

            const gainNode = audioContext.createGain();
            gainNode.gain.value = 1
            source.connect(gainNode);
            const biquadFilter = audioContext.createBiquadFilter(); //滤波节点，过滤指定频段波形
            gainNode.connect(biquadFilter)

            const waveShaper = audioContext.createWaveShaper();

            biquadFilter.connect(waveShaper);

            const convolver = audioContext.createConvolver(); //混音节点
            convolver.buffer = audioBuffer

            waveShaper.connect(convolver)

            let analyser = audioContext.createAnalyser(); // 创建AnalyserNode节点
            analyser.fftSize = 256;

            convolver.connect(analyser);
            analyser.connect(audioContext.destination);

            // 可以对音频做任何控制
            source.start(0); //立即播放
            flushData(analyser)

        }
        function flushData(analyser) {
            var globalID;
            function render() {
                let dataArray = new Uint8Array(analyser.frequencyBinCount); //analyser.frequencyBinCount为analyser.fftSize设置的一半
                // 获取时域数据（对于波形显示，通常使用getByteTimeDomainData）
                // analyser.getByteTimeDomainData(dataArray);
                // console.log("Time domain data:", dataArrayTime);

                // 获取频域数据
                analyser.getByteFrequencyData(dataArray);
                console.log("Frequency domain data:", dataArray);
                //Todo 通过dataArray数据实现可视化效果
                // 可控制canvas css

                globalID = requestAnimationFrame(render);
            };
            globalID = requestAnimationFrame(render);
        }

        // mediaStream
        function mediaAudio() {

            // 创建一个与MediaStream关联的Source Node
            // const sourceNode = audioContext.createMediaStreamSource(mediaStream);
            const sourceNode = audioContext.createMediaStreamSource(audio2.captureStream());
            // 这里可以添加更多音频处理节点，比如增益控制、分析等
            // 示例：添加一个增益节点调节音量
            const gainNode = audioContext.createGain();
            gainNode.gain.value = 0.1
            sourceNode.connect(gainNode);
            let analyser = audioContext.createAnalyser(); // 创建AnalyserNode节点
            analyser.fftSize = 256;
            gainNode.connect(analyser);
            // 可选：如果你想让音频通过扬声器播放，还需要将AnalyserNode连接到destination
            // analyser.connect(audioContext.destination);

            // flushData(analyser)
            // 开始音频处理流程
            // 注意：如果是处理实时流，不需要手动开始，连接后自动播放
        }

        function audioLabel() {
            const sourceNode = audioContext.createMediaElementSource(audio);
            // 将媒体元素源节点连接到输出目的地，通常是扬声器或耳机
            // sourceNode.connect(audioContext.destination);

            const gainNode = audioContext.createGain();
            gainNode.gain.value = 0.1
            sourceNode.connect(gainNode);
            audio.volume = 0.5

            let analyser = audioContext.createAnalyser(); // 创建AnalyserNode节点
            analyser.fftSize = 256;
            gainNode.connect(analyser);
            analyser.connect(audioContext.destination);
            flushData(analyser)
            audio.play()
        }


    </script>
</body>

</html>