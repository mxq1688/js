<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>

<body>
    <div onclick="fetchAudio()">fetchAudio</div>
    <div onclick="mediaAudio()">mediaAudio</div>
    <div onclick="audioLabel()">audioLabel</div>
    <audio id="audio2" muted autoplay controls></audio>
    <video id="video" muted autoplay controls style="width: 150px;height: 150px;"></video>
    <!-- <canvas id="canvas"></canvas> -->
    <audio id="audio" autoplay controls src="http://localhost:3000/1.mp3"></audio>
    <!-- <video id="video" autoplay controls src="http://localhost:3000/1.mp4"></video> -->

    <div>
        <div>MediaRecorder</div>
        <button onclick="mStart()">录制开始</button>
        <button onclick="mPause()">暂停</button>
        <button onclick="mResume()">继续</button>
        <button onclick="mStop()">录制结束</button>
        <button onclick="mRequest()">请求录制数据</button>
    </div>
    <div>
        <div>AudioRecorder</div>
        <button onclick="aStart()">录制开始</button>
        <button onclick="aPause()">暂停</button>
        <button onclick="aResume()">继续</button>
        <button onclick="aStop()">录制结束</button>
    </div>
    <script type="module">
        import Resampler from './resampler.js'
        window.Resampler = Resampler
    </script>

    <script>
        let mediaStream
        var audioContext
        let mediaRecorder
        let audioRecorder
        let audioSource
        navigator.mediaDevices.getUserMedia({ audio: true })
            .then(function (stream) {
                audioContext = new (window.AudioContext || window.webkitAudioContext())();
                /* 使用这个 stream */
                mediaStream = stream
                audio2.srcObject = stream
                video.srcObject = stream

                mediaRecorderInit()
                audioRecorderInit()
            })
            .catch(function (err) {
                console.log(err);
                /* 处理 error */
                switch (error.message || error.name) {
                    case 'PERMISSION_DENIED':
                    case 'PermissionDeniedError':
                        console.info('用户拒绝提供信息。')
                        break
                    case 'NOT_SUPPORTED_ERROR':
                    case 'NotSupportedError':
                        console.info('浏览器不支持硬件设备。')
                        break
                    case 'MANDATORY_UNSATISFIED_ERROR':
                    case 'MandatoryUnsatisfiedError':
                        console.info('无法发现指定的硬件设备。')
                        break
                    default:
                        console.info('无法打开麦克风。异常信息:' + (error.code || error.name))
                        break
                }
            });

        let recordedChunks = [];
        function audioRecorderInit() {
            audioSource = audioContext.createMediaStreamSource(mediaStream) //也可以是其他形式

            const createScript =
                audioContext.createScriptProcessor || audioContext.createJavaScriptNode;
            audioRecorder = createScript.apply(audioContext, [4096, 2, 2]);

            // 当有音频数据时，调用此函数
            audioRecorder.onaudioprocess = function (event) {
                // event.inputBuffer 是一个 AudioBuffer 对象，包含 PCM 数据
                const leftChannelData = event.inputBuffer.getChannelData(0);
                const rightChannelData = event.inputBuffer.getChannelData(1);
                const inputBuffer = []
                // 将左右声道数据合并到inputBuffer中
                for (let i = 0; i < leftChannelData.length; i++) {
                    inputBuffer.push(leftChannelData[i]);
                    inputBuffer.push(rightChannelData[i]);
                }
                const sAudioData = new Float32Array(inputBuffer)
                console.log(sAudioData, 111);
                const array = new Float32Array(leftChannelData.length);
                array.set(leftChannelData);
                recordedChunks.push(array);

                // audioData.input(leftChannelData);
                // const resampledWavData = audioData.resample(leftChannelData);
                // console.log(buffer, resampledWavData, audioContext.sampleRate);
                // const resampled16BitsWavData = audioData.quantize(resampledWavData, 16);
                // console.log(resampled16BitsWavData.buffer);
            };

        }

        function aStart() {
            audioSource.connect(audioRecorder);
            audioRecorder.connect(audioContext.destination);
        }
        function aPause() {
            audioRecorder.disconnect();
            audioContext.suspend();
        }
        function aResume() {
            audioContext.resume();
            audioSource.connect(audioRecorder);
            audioRecorder.connect(audioContext.destination);
        }
        function aStop() {
            audioRecorder.disconnect();
            audioContext.close();
            mediaStream?.getTracks().forEach((t) => t.stop());
            dellAudioData()
        }
        function dellAudioData() {
            // 创建一个完整的 AudioBuffer，用于保存录制的音频
            const recordingLength = recordedChunks.reduce((acc, chunk) => acc + chunk.length, 0);
            console.log('dellAudioData', recordedChunks, recordingLength, audioContext.sampleRate);
            const audioBuffer = audioContext.createBuffer(2, recordingLength, audioContext.sampleRate);

            let srcStart = 0;
            for (const chunk of recordedChunks) {
                audioBuffer.copyToChannel(chunk, 0, srcStart);
                srcStart += chunk.length;
            }
            console.log('audioBuffer', audioBuffer, audioBuffer.getChannelData(0), audioBuffer.getChannelData(1));

        }

        function mediaRecorderInit() {
            const mimeType = 'video/webm;codecs=vp9';
            mediaRecorder = new MediaRecorder(mediaStream, { mimeType });
            console.log('isTypeSupported', MediaRecorder.isTypeSupported(mimeType), mediaRecorder.mimeType);
            const chunks = []
            mediaRecorder.ondataavailable = function (event) {
                if (event.data && event.data.size > 0) {
                    console.log(event.data, 'data');
                    chunks.push(event.data); // 将数据块添加到数组中
                }
            };
            mediaRecorder.onstop = function () {
                const blob = new Blob(chunks, { type: chunks[0].type }); // 将数据块组合成 Blob
                console.log(blob, 111);
                const url = URL.createObjectURL(blob); // 创建 Blob 的 URL
                const a = document.createElement('a'); // 创建一个 <a> 元素用于下载
                a.href = url;
                a.download = 'recording.webm';
                a.click(); // 触发下载
            }
        }
        function mStart() {
            mediaRecorder.start(3000);
        }
        function mPause() {
            mediaRecorder.pause();
        }
        function mResume() {
            mediaRecorder.resume();
        }
        function mStop() {
            mediaStream?.getTracks().forEach((t) => t.stop());
            mediaRecorder.stop();
        }
        function mRequest() {
            mediaRecorder.requestData();
        }
        // 失真
        function makeDistortionCurve(amount) {
            var k = typeof amount === "number" ? amount : 50,
                n_samples = 44100,
                curve = new Float32Array(n_samples),
                deg = Math.PI / 180,
                i = 0,
                x;
            for (; i < n_samples; ++i) {
                x = (i * 2) / n_samples - 1;
                curve[i] = ((3 + k) * x * 20 * deg) / (Math.PI + k * Math.abs(x));
            }
            return curve;
        }
        async function fetchAudio() {
            let source = audioContext.createBufferSource(); // 创建音频源头节点
            const res = await fetch('http://localhost:3000/1.mp4');
            const arrayBuffer = await res.arrayBuffer(); // byte array字节数组
            // const audioBuffer = await audioContext.decodeAudioData(arrayBuffer, function (decodeData) {
            //     return decodeData;
            // });
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer).then((decodeData) => {
                return decodeData;
            })
            source.buffer = audioBuffer; // 设置数据
            source.loop = true; //设置，循环播放

            const gainNode = audioContext.createGain();
            gainNode.gain.value = 1
            source.connect(gainNode);
            const biquadFilter = audioContext.createBiquadFilter(); //滤波节点，过滤指定频段波形
            gainNode.connect(biquadFilter)

            const waveShaper = audioContext.createWaveShaper();

            biquadFilter.connect(waveShaper);

            const convolver = audioContext.createConvolver(); //混音节点
            convolver.buffer = audioBuffer

            waveShaper.connect(convolver)

            let analyser = audioContext.createAnalyser(); // 创建AnalyserNode节点
            analyser.fftSize = 256;

            convolver.connect(analyser);
            analyser.connect(audioContext.destination);

            // 可以对音频做任何控制
            source.start(0); //立即播放
            flushData(analyser)

        }
        function flushData(analyser) {
            var globalID;
            function render() {
                let dataArray = new Uint8Array(analyser.frequencyBinCount); //analyser.frequencyBinCount为analyser.fftSize设置的一半
                // 获取时域数据（对于波形显示，通常使用getByteTimeDomainData）
                // analyser.getByteTimeDomainData(dataArray);
                // console.log("Time domain data:", dataArrayTime);

                // 获取频域数据
                analyser.getByteFrequencyData(dataArray);
                console.log("Frequency domain data:", dataArray);
                //Todo 通过dataArray数据实现可视化效果
                // 可控制canvas css

                globalID = requestAnimationFrame(render);
            };
            globalID = requestAnimationFrame(render);
        }

        // mediaStream
        function mediaAudio() {

            // 创建一个与MediaStream关联的Source Node
            // const sourceNode = audioContext.createMediaStreamSource(mediaStream);
            const sourceNode = audioContext.createMediaStreamSource(audio2.captureStream());
            // 这里可以添加更多音频处理节点，比如增益控制、分析等
            // 示例：添加一个增益节点调节音量
            const gainNode = audioContext.createGain();
            gainNode.gain.value = 0.1
            sourceNode.connect(gainNode);
            let analyser = audioContext.createAnalyser(); // 创建AnalyserNode节点
            analyser.fftSize = 256;
            gainNode.connect(analyser);
            // 可选：如果你想让音频通过扬声器播放，还需要将AnalyserNode连接到destination
            // analyser.connect(audioContext.destination);

            // flushData(analyser)
            // 开始音频处理流程
            // 注意：如果是处理实时流，不需要手动开始，连接后自动播放
        }

        function audioLabel() {
            const sourceNode = audioContext.createMediaElementSource(audio);
            // 将媒体元素源节点连接到输出目的地，通常是扬声器或耳机
            // sourceNode.connect(audioContext.destination);

            const gainNode = audioContext.createGain();
            gainNode.gain.value = 0.1
            sourceNode.connect(gainNode);
            audio.volume = 0.5

            let analyser = audioContext.createAnalyser(); // 创建AnalyserNode节点
            analyser.fftSize = 256;
            gainNode.connect(analyser);
            analyser.connect(audioContext.destination);
            flushData(analyser)
            audio.play()
        }


    </script>
</body>

</html>